# The Devil Wears Data

> *Because that "pile of stuff" was never *just* stuff â€” it was data waiting to be modeled.*

## TL;DR

I built an open, longitudinal dataset of 14 luxury fashion houses (2018â€‘2024) â€” creative directors, runway calendars, sentiment, revenue, and more â€” then showed how causal inference (synthetic control) can quantify the impact of Pharrell Williams at LouisÂ Vuitton.  Clone the repo, open the RÂ notebook, and run `make all` to reproduce every figure.

---

## TableÂ ofÂ Contents

1. [Why This Matters](#why-this-matters)
2. [Dataset Schema](#dataset-schema)
3. [Methodology](#methodology)
4. [Repository Structure](#repository-structure)
5. [Quickâ€‘Start](#quick-start)
6. [Reproducing the Analysis](#reproducing-the-analysis)
7. [LimitationsÂ &Â Roadmap](#limitations--roadmap)
8. [Citation](#citation)
9. [License](#license)
10. [Contact](#contact)

---

## Why This Matters

Fashion is a **\$1.7â€¯T** industry that still relies on anecdotes for strategic decisions.  Data exists â€” scattered across runway schedules, glossy editorials, and SEC filings â€” but no public, researchâ€‘grade panel ties it all together.

This project delivers that missing link so that:

* **Researchers** can run timeâ€‘series and causal inference without months of scraping.
* **Students** can learn econometrics on a fun, culturally rich topic.
* **Brands & analysts** can benchmark creative decisions against measurable outcomes.

---

## DatasetÂ Schema

| Category        | Variable                                                 | Type         | Notes                              |
| --------------- | -------------------------------------------------------- | ------------ | ---------------------------------- |
| Identity        | `house`                                                  | factor       | 14 maisons in v1.0                 |
| Time            | `year`, `season`                                         | int / factor | `season âˆˆ {ss, fw}`                |
| Governance      | `creative_director`, `director_years`, `director_houses` | chr / int    | Tenure + career breadth            |
| Geography       | `home_base`, `parent_group`                              | factor       | Links to HQ & conglomerate         |
| Runway Presence | `paris`, `milan`, `new_york`, `london`                   | 0/1          | Major fashion weeks                |
| Culture         | `met_gala`                                               | 0/1          | Redâ€‘carpet signal                  |
| Outcome         | `fashion_magazine_sentiment`                             | num          | VADER compound score               |
| FinanceÂ â€        | `seasonal_revenue`, `employees`                          | num          | Publicly reported, where available |

â€ Â Financial coverage currently limited to houses with public filings.  See `data/codebook.csv` for detailed metadata.

Raw CSV lives in [`data/CapstoneÂ FinalÂ DraftÂ Data.csv`](./data). A rendered codebook is in `/docs`.

---

## Methodology

### 1Â Â·Â Sentiment Pipeline

* Crawl Vogue, BOF, W, Harperâ€™sÂ Bazaar via GoogleÂ CSE + NewsÂ API.
* Extract article text with *newspaper3k*.
* Score with **VADER**; aggregate by houseâ€‘season.
* Store as tidy panel (`houseÂ Ã—Â season`).

Python implementation lives in [`scripts/social_media_sentiment.py`](./scripts).  A sample notebook shows exploratory plots.

### 2Â Â·Â Causal Demo

In `analysis/synth_louis_vuitton.R` I replicate the synthetic control example from the report:

```r
source("analysis/synth_louis_vuitton.R")
```

The script:

1. Converts `year + season` to halfâ€‘year time.
2. Builds a donor pool (13 houses).
3. Matches on fashionâ€‘week presence + director tenure preâ€‘2023.
4. Estimates ATT of Pharrellâ€™s appointment on magazine sentiment.

Reproduced figure:

```
Impact of Pharrell Williams at LouisÂ Vuitton (2023â€‘2024)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ actual (LV) â”€â”€â”€â”€â”€â”€â”
â”‚                             â”‚
â””â”€â”€ synthetic control (LVâ€§) â”€â”€â”˜
```

---

## RepositoryÂ Structure

```
â”œâ”€â”€ data/                  # Raw & processed CSVs + codebook
â”œâ”€â”€ scripts/               # Python crawlers & sentiment pipeline
â”œâ”€â”€ analysis/              # R notebooks & .R scripts for causal work
â”œâ”€â”€ docs/                  # PDF report, figures, and slides
â”œâ”€â”€ assets/                # Logos & banner images
â””â”€â”€ README.md              # You are here
```

---

## QuickÂ Start

```bash
# 1Â Â·Â Clone
$ git clone https://github.com/yourusername/devil-wears-data.git
$ cd devil-wears-data

# 2Â Â·Â Set up R (â‰¥ 4.3) & install deps
$ R -e "install.packages(c('Synth','dplyr','ggplot2','tidyr','readr'))"

# 3Â Â·Â (Optional) Python sentiment pipeline
$ pip install -r requirements.txt
$ python scripts/social_media_sentiment.py  # writes data/sentiment.csv
```

A `Makefile` is included for oneâ€‘command reproduction: `make all`.

---

## Reproducing the Analysis

1. Run the sentiment pipeline (or use the preâ€‘computed CSV).
2. Open `analysis/synth_louis_vuitton.Rmd` and knit to HTML.
3. All figures and tables from the capstone paper will be generated under `/docs`.

---

## LimitationsÂ &Â Roadmap

* **Coverage**: v1.0 tracks 14 houses; goal is 30+ by 2026.
* **Sentiment Model**: VADER is fashionâ€‘agnostic; exploring FinBERTâ€‘style fineâ€‘tuning.
* **Financials**: Scraping PDF annual reports to improve revenue granularity.
* **CI/CD**: Automate weekly sentiment refresh via GitHub Actions.

---

## Citation

```bibtex
@misc{nguyen2024devil,
  title  = {The Devil Wears Data},
  author = {Nguyen, An},
  year   = {2024},
  note   = {GitHubÂ repository},
  url    = {https://github.com/yourusername/devil-wears-data}
}
```

---

## License

Released under the MIT License â€” see [`LICENSE`](./LICENSE).

---

## Contact

Have questions or want to collaborate?

ğŸ‘—Â AnÂ Nguyen  Â·  [an@uni.minerva.edu](mailto:an@uni.minerva.edu)  Â·  [@causalnotcasual](https://www.instagram.com/causalnotcasual)

*Data is the new cerulean.*
