{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae22b8a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.8/site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./opt/anaconda3/lib/python3.8/site-packages (4.11.1)\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in ./opt/anaconda3/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.8/site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/lib/python3.8/site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.8/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 vaderSentiment pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c351f6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Take a trip inside the debut collection</td>\n",
       "      <td>MagazineDaniel Lee’s Debut Collection for Burb...</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  headline  \\\n",
       "0  Take a trip inside the debut collection   \n",
       "\n",
       "                                             content  compound_score  \\\n",
       "0  MagazineDaniel Lee’s Debut Collection for Burb...           0.926   \n",
       "\n",
       "   pos_score  neu_score  neg_score  \n",
       "0      0.036      0.964        0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to fetch articles from a blog or magazine\n",
    "def fetch_blog_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Adjust this according to the specific HTML structure of the site\n",
    "    articles = soup.find_all('article')\n",
    "    return articles\n",
    "\n",
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Function to format sentiment results into a DataFrame\n",
    "def analyze_vogue_sentiment():\n",
    "    vogue_url = 'https://www.vogue.com/article/daniel-lee-debut-burberry-collection'\n",
    "    articles = fetch_blog_content(vogue_url)\n",
    "    \n",
    "    # List to store article data\n",
    "    sentiment_data = []\n",
    "    \n",
    "    for article in articles:\n",
    "        headline = article.find('h2').get_text() if article.find('h2') else 'No Headline'\n",
    "        content = article.get_text()\n",
    "        \n",
    "        # Perform sentiment analysis\n",
    "        sentiment_score = analyze_sentiment(content)\n",
    "        \n",
    "        # Append data as a dictionary (row of the DataFrame)\n",
    "        sentiment_data.append({\n",
    "            'headline': headline,\n",
    "            'content': content,\n",
    "            'compound_score': sentiment_score['compound'],\n",
    "            'pos_score': sentiment_score['pos'],\n",
    "            'neu_score': sentiment_score['neu'],\n",
    "            'neg_score': sentiment_score['neg']\n",
    "        })\n",
    "    \n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    sentiment_df = pd.DataFrame(sentiment_data)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "# Running the function and saving the results into a DataFrame\n",
    "vogue_sentiment_df = analyze_vogue_sentiment()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "vogue_sentiment_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade51a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/article/magnesium-rich-foods', '/article/dior-beauty-ss25', '/article/dior-spa-at-the-plaza-athenee', '/article/brigitte-macron-first-dior-show', '/article/dior-haute-couture-fw-24-beauty', '/article/probiotics', '/article/best-cooking-oil', '/fashion-shows', '/article/dior-spa-eden-roc']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to get all relevant URLs from Vogue\n",
    "def fetch_vogue_urls(search_term):\n",
    "    # Vogue search URL with your query (adjust 'burberry+daniel+lee' as needed)\n",
    "    search_url = f\"https://www.vogue.com/search?q={search_term}\"\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # List to store URLs\n",
    "    urls = []\n",
    "\n",
    "    # Vogue often uses <a> tags with an href attribute for their articles\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        # Ensure URLs are articles (i.e., contain 'fashion-shows', 'article', etc.)\n",
    "        if 'article' in href or 'fashion-shows' in href:\n",
    "            urls.append(href)\n",
    "\n",
    "    return urls\n",
    "\n",
    "# Fetch URLs related to Burberry and Daniel Lee\n",
    "search_term = \"kriss van asche dior\"\n",
    "vogue_urls = fetch_vogue_urls(search_term)\n",
    "\n",
    "# Optional: Filter only unique URLs\n",
    "vogue_urls = list(set(vogue_urls))\n",
    "\n",
    "# Output the URLs\n",
    "print(vogue_urls)\n",
    "\n",
    "# You can now analyze the URLs or pass them into another function for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eef5398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.vogue.com/article/dior-beauty-ss25</td>\n",
       "      <td>To revisit this article, visit My Profile, the...</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.vogue.com/article/lee-kiefer-paris...</td>\n",
       "      <td>To revisit this article, visit My Profile, the...</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.vogue.com/article/brigitte-macron-...</td>\n",
       "      <td>To revisit this article, visit My Profile, the...</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.vogue.com/'/article/vogue-club/get...</td>\n",
       "      <td>Find anything you save across the site in your...</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.vogue.com/article/burberry-reopens...</td>\n",
       "      <td>To revisit this article, visit My Profile, the...</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.vogue.com/article/dior-beauty-ss25   \n",
       "1  https://www.vogue.com/article/lee-kiefer-paris...   \n",
       "2  https://www.vogue.com/article/brigitte-macron-...   \n",
       "3  https://www.vogue.com/'/article/vogue-club/get...   \n",
       "4  https://www.vogue.com/article/burberry-reopens...   \n",
       "\n",
       "                                             content  compound_score  \\\n",
       "0  To revisit this article, visit My Profile, the...          0.9946   \n",
       "1  To revisit this article, visit My Profile, the...          0.9989   \n",
       "2  To revisit this article, visit My Profile, the...          0.9774   \n",
       "3  Find anything you save across the site in your...          0.9100   \n",
       "4  To revisit this article, visit My Profile, the...          0.9962   \n",
       "\n",
       "   pos_score  neu_score  neg_score  \n",
       "0      0.107      0.864      0.029  \n",
       "1      0.147      0.804      0.048  \n",
       "2      0.073      0.927      0.000  \n",
       "3      0.117      0.883      0.000  \n",
       "4      0.073      0.923      0.004  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# List of URLs you got from the search or scraping\n",
    "vogue_urls = [\n",
    "    \"https://www.vogue.com/article/dior-beauty-ss25\",\n",
    "    \"https://www.vogue.com/article/lee-kiefer-paris-games-interview\",\n",
    "    \"https://www.vogue.com/article/brigitte-macron-first-dior-show\",\n",
    "    \"https://www.vogue.com/'/article/vogue-club/get-to-know-kristin-vartan\",\n",
    "    \"https://www.vogue.com/article/burberry-reopens-57th-street-new-york-flagship-daniel-lee-interview\"\n",
    "    #\"https:///article/dior-spa-eden-roc\"\n",
    "    \n",
    "    \n",
    "    # Add more URLs here...\n",
    "]\n",
    "\n",
    "#bof_urls = [\n",
    "    #\"https://www.businessoffashion.com/opinions/luxury/does-burberry-have-the-wrong-strategy/\",\n",
    "    #\"https://www.businessoffashion.com/news/luxury/burberry-plunges-as-company-slashes-profit-forecast/\",\n",
    "    #\"https://www.businessoffashion.com/articles/luxury/burberry-reveals-new-logo-first-campaign-by-daniel-lee-ahead-of-debut-show/\",\n",
    "    #\"https://www.businessoffashion.com/opinions/luxury/burberry-desperately-needs-the-return-of-britpop/\"\n",
    "#]\n",
    "\n",
    "# Function to fetch article content from a given URL\n",
    "def fetch_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Adjust this based on the structure of the articles on Vogue\n",
    "    # Usually, articles are within <p> tags\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = ' '.join([para.get_text() for para in paragraphs])\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Function to perform sentiment analysis on the content\n",
    "def analyze_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Function to analyze sentiment for each article from the URLs\n",
    "def analyze_vogue_articles(urls):\n",
    "    sentiment_data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        content = fetch_article_content(url)\n",
    "        sentiment_score = analyze_sentiment(content)\n",
    "        \n",
    "        # Append the results to the sentiment_data list\n",
    "        sentiment_data.append({\n",
    "            'url': url,\n",
    "            'content': content,\n",
    "            'compound_score': sentiment_score['compound'],\n",
    "            'pos_score': sentiment_score['pos'],\n",
    "            'neu_score': sentiment_score['neu'],\n",
    "            'neg_score': sentiment_score['neg']\n",
    "        })\n",
    "    \n",
    "    # Create a pandas DataFrame from the sentiment data\n",
    "    sentiment_df = pd.DataFrame(sentiment_data)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "# Run the analysis on the list of URLs\n",
    "vogue_sentiment_df = analyze_vogue_articles(vogue_urls)\n",
    "\n",
    "#bof_sentiment_df = analyze_vogue_articles(bof_urls)\n",
    "\n",
    "# Print the DataFrame with sentiment scores\n",
    "# vogue_sentiment_df.head()\n",
    "vogue_sentiment_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a9dffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnewspaper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Article\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from newspaper import Article\n",
    "import time\n",
    "\n",
    "class BrandSentimentAnalyzer:\n",
    "    def __init__(self, google_api_key, news_api_key):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.google_api_key = google_api_key\n",
    "        self.news_api_key = news_api_key\n",
    "        self.google_service = build('customsearch', 'v1', developerKey=google_api_key)\n",
    "\n",
    "    def fetch_google_results(self, query, start_date=None, end_date=None, max_results=50):\n",
    "        \"\"\"\n",
    "        Fetch results from Google Custom Search API\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        start_index = 1\n",
    "\n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                # Build date range string if dates are provided\n",
    "                date_range = ''\n",
    "                if start_date and end_date:\n",
    "                    date_range = f' after:{start_date} before:{end_date}'\n",
    "                \n",
    "                search_query = query + date_range\n",
    "                \n",
    "                response = self.google_service.cse().list(\n",
    "                    q=search_query,\n",
    "                    cx='YOUR_SEARCH_ENGINE_ID',  # You'll need to create this\n",
    "                    start=start_index\n",
    "                ).execute()\n",
    "\n",
    "                if 'items' not in response:\n",
    "                    break\n",
    "\n",
    "                for item in response['items']:\n",
    "                    results.append({\n",
    "                        'title': item.get('title', ''),\n",
    "                        'link': item.get('link', ''),\n",
    "                        'snippet': item.get('snippet', ''),\n",
    "                        'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('article:published_time', '')\n",
    "                    })\n",
    "\n",
    "                if len(response['items']) < 10:  # Less than maximum results per page\n",
    "                    break\n",
    "\n",
    "                start_index += 10\n",
    "                time.sleep(1)  # Respect rate limits\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Google search: {e}\")\n",
    "                break\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def fetch_news_api_results(self, query, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Fetch results from News API\n",
    "        \"\"\"\n",
    "        url = 'https://newsapi.org/v2/everything'\n",
    "        \n",
    "        params = {\n",
    "            'q': query,\n",
    "            'apiKey': self.news_api_key,\n",
    "            'language': 'en',\n",
    "            'sortBy': 'relevancy',\n",
    "            'pageSize': 100\n",
    "        }\n",
    "        \n",
    "        if start_date:\n",
    "            params['from'] = start_date\n",
    "        if end_date:\n",
    "            params['to'] = end_date\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            articles = response.json()['articles']\n",
    "            \n",
    "            results = [{\n",
    "                'title': article['title'],\n",
    "                'link': article['url'],\n",
    "                'snippet': article['description'],\n",
    "                'date': article['publishedAt'],\n",
    "                'source': article['source']['name']\n",
    "            } for article in articles]\n",
    "            \n",
    "            return pd.DataFrame(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def fetch_article_content(self, url):\n",
    "        \"\"\"\n",
    "        Fetch and parse article content using newspaper3k\n",
    "        \"\"\"\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            return article.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching article content from {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text using VADER\n",
    "        \"\"\"\n",
    "        return self.analyzer.polarity_scores(text)\n",
    "\n",
    "    def analyze_brand_sentiment(self, brand_name, creative_director=None, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Comprehensive brand sentiment analysis\n",
    "        \"\"\"\n",
    "        # Construct search query\n",
    "        query = f\"{brand_name}\"\n",
    "        if creative_director:\n",
    "            query += f\" {creative_director}\"\n",
    "\n",
    "        # Get results from both APIs\n",
    "        google_results = self.fetch_google_results(query, start_date, end_date)\n",
    "        news_results = self.fetch_news_api_results(query, start_date, end_date)\n",
    "\n",
    "        # Combine results\n",
    "        all_results = pd.concat([google_results, news_results]).drop_duplicates(subset=['link'])\n",
    "\n",
    "        # Analyze sentiment for each article\n",
    "        sentiment_data = []\n",
    "        for _, row in all_results.iterrows():\n",
    "            content = self.fetch_article_content(row['link'])\n",
    "            sentiment = self.analyze_sentiment(content)\n",
    "            \n",
    "            sentiment_data.append({\n",
    "                'date': row['date'],\n",
    "                'title': row['title'],\n",
    "                'url': row['link'],\n",
    "                'snippet': row['snippet'],\n",
    "                'content': content,\n",
    "                'compound_score': sentiment['compound'],\n",
    "                'pos_score': sentiment['pos'],\n",
    "                'neu_score': sentiment['neu'],\n",
    "                'neg_score': sentiment['neg']\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(sentiment_data)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    analyzer = BrandSentimentAnalyzer(\n",
    "        google_api_key='YOUR_GOOGLE_API_KEY',\n",
    "        news_api_key='YOUR_NEWS_API_KEY'\n",
    "    )\n",
    "    \n",
    "    # Analyze sentiment for Burberry under Daniel Lee\n",
    "    results = analyzer.analyze_brand_sentiment(\n",
    "        brand_name='Burberry',\n",
    "        creative_director='Daniel Lee',\n",
    "        start_date='2023-01-01',\n",
    "        end_date='2024-01-01'\n",
    "    )\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results.to_csv('burberry_sentiment_analysis.csv', index=False)\n",
    "    \n",
    "    # Basic analysis\n",
    "    print(\"\\nAverage Sentiment Scores:\")\n",
    "    print(f\"Compound: {results['compound_score'].mean():.3f}\")\n",
    "    print(f\"Positive: {results['pos_score'].mean():.3f}\")\n",
    "    print(f\"Neutral: {results['neu_score'].mean():.3f}\")\n",
    "    print(f\"Negative: {results['neg_score'].mean():.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdb46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
